{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e616a902",
   "metadata": {},
   "source": [
    "# PyGraSPI Introduction\n",
    "\n",
    "This notebook provides an introduction to using the PyGraSPI API. PyGraSPI provides a set of features or descriptors from sample microstructures. It is an alternative to using 2-point stats in homogenization workflows for materials science AI applications. PyGraSPI currently provides a function, `make_descriptors`, that takes a set of microstructures and returns a set of descriptors in a Pandas dataframe. PyGraSPI returns two main categories of descriptors. The first is based on the graph network generated from the microstructure where the graph nodes are colored based on the material phase. This method provides descriptors such as vertex count, tortuosity and connected components. The second method, based on the skeleton of the graph, provides features concerned with the internal cycles and intersections in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c08cd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"PyGraSPI Intro\n",
    "\"\"\"\n",
    "\n",
    "import zipfile\n",
    "\n",
    "import dask.array as da\n",
    "import numpy as np\n",
    "import pandas\n",
    "from pymks import solve_cahn_hilliard\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from toolz.curried import curry, pipe\n",
    "\n",
    "from pygraspi.combined_descriptors import make_descriptors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8b8e70",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "The data used here consists of 573 artificially generated microstructures from Cahn-Hilliard simulations with 401x101 shaped grids (see [Jivani et al.](https://doi.org/10.1016/j.commatsci.2021.110409) for more details). Each data sample is generated with a random initial condition and with different initial volume fractions and interaction parameters and run for varying durations. On unzipping the data in `data/cahn-hilliard.zip` there are files of the type `data_X.XXX_Y.Y_NNNNNNN.txt` where the `X.XXX` refer to the volume fraction and the `Y.Y` refer to the interaction parameter values. The `NNNNNN` denotes the number of time steps reached for that particular sample. Note that files with corresponding volume fractions and interaction parameters are from the same simulation (just with varying duration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4995fb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 401, 101)\n"
     ]
    }
   ],
   "source": [
    "@curry\n",
    "def read_data(zip_stream_, file_name):\n",
    "    \"\"\"Read a single CSV file\"\"\"\n",
    "    return np.array(\n",
    "        pandas.read_csv(\n",
    "            zip_stream_.open(file_name, \"r\"), delimiter=\" \", header=None\n",
    "        ).swapaxes(0, 1)\n",
    "    )\n",
    "\n",
    "\n",
    "with zipfile.ZipFile(\"data/cahn-hilliard.zip\", \"r\") as zip_stream:\n",
    "    data = np.array(\n",
    "        list(\n",
    "            # pylint: disable=no-value-for-parameter\n",
    "            map(read_data(zip_stream), zip_stream.namelist()[:3])\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06942ad0",
   "metadata": {},
   "source": [
    "Currently, we are only using 3 samples as the implementation uses NetworkX, which is extremely slow for this category of calculations. The new implementation will use Graph-tool, which is considerably more efficient for these calculations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e1a11ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>branch_length_a</th>\n",
       "      <th>branch_length_b</th>\n",
       "      <th>dist_to_interface_avg_a</th>\n",
       "      <th>dist_to_interface_avg_b</th>\n",
       "      <th>dist_to_interface_max_a</th>\n",
       "      <th>dist_to_interface_max_b</th>\n",
       "      <th>dist_to_interface_min_a</th>\n",
       "      <th>dist_to_interface_min_b</th>\n",
       "      <th>f_skeletal_pixels_a</th>\n",
       "      <th>f_skeletal_pixels_b</th>\n",
       "      <th>...</th>\n",
       "      <th>distance_to_top_1</th>\n",
       "      <th>interfacial_area</th>\n",
       "      <th>phase_0_cc</th>\n",
       "      <th>phase_0_count</th>\n",
       "      <th>phase_0_interface</th>\n",
       "      <th>phase_1_cc</th>\n",
       "      <th>phase_1_count</th>\n",
       "      <th>phase_1_interface</th>\n",
       "      <th>top_boundary_count_0</th>\n",
       "      <th>top_boundary_count_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44.71</td>\n",
       "      <td>39.28</td>\n",
       "      <td>9.25</td>\n",
       "      <td>9.50</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>20.591260</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>...</td>\n",
       "      <td>6.059710</td>\n",
       "      <td>5426</td>\n",
       "      <td>1</td>\n",
       "      <td>19600</td>\n",
       "      <td>2697</td>\n",
       "      <td>1</td>\n",
       "      <td>20901</td>\n",
       "      <td>2729</td>\n",
       "      <td>54</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46.08</td>\n",
       "      <td>33.81</td>\n",
       "      <td>9.63</td>\n",
       "      <td>10.36</td>\n",
       "      <td>16.124515</td>\n",
       "      <td>20.808652</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>...</td>\n",
       "      <td>6.303166</td>\n",
       "      <td>5082</td>\n",
       "      <td>1</td>\n",
       "      <td>19341</td>\n",
       "      <td>2518</td>\n",
       "      <td>1</td>\n",
       "      <td>21160</td>\n",
       "      <td>2564</td>\n",
       "      <td>47</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42.78</td>\n",
       "      <td>32.97</td>\n",
       "      <td>10.72</td>\n",
       "      <td>11.79</td>\n",
       "      <td>17.691806</td>\n",
       "      <td>21.587033</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>...</td>\n",
       "      <td>6.702417</td>\n",
       "      <td>4508</td>\n",
       "      <td>2</td>\n",
       "      <td>19233</td>\n",
       "      <td>2230</td>\n",
       "      <td>1</td>\n",
       "      <td>21268</td>\n",
       "      <td>2278</td>\n",
       "      <td>49</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   branch_length_a  branch_length_b  dist_to_interface_avg_a  \\\n",
       "0            44.71            39.28                     9.25   \n",
       "1            46.08            33.81                     9.63   \n",
       "2            42.78            32.97                    10.72   \n",
       "\n",
       "   dist_to_interface_avg_b  dist_to_interface_max_a  dist_to_interface_max_b  \\\n",
       "0                     9.50                15.000000                20.591260   \n",
       "1                    10.36                16.124515                20.808652   \n",
       "2                    11.79                17.691806                21.587033   \n",
       "\n",
       "   dist_to_interface_min_a  dist_to_interface_min_b  f_skeletal_pixels_a  \\\n",
       "0                      1.0                      1.0                 0.03   \n",
       "1                      1.0                      1.0                 0.03   \n",
       "2                      4.0                      1.0                 0.03   \n",
       "\n",
       "   f_skeletal_pixels_b  ...  distance_to_top_1  interfacial_area  phase_0_cc  \\\n",
       "0                 0.02  ...           6.059710              5426           1   \n",
       "1                 0.02  ...           6.303166              5082           1   \n",
       "2                 0.02  ...           6.702417              4508           2   \n",
       "\n",
       "   phase_0_count  phase_0_interface  phase_1_cc  phase_1_count  \\\n",
       "0          19600               2697           1          20901   \n",
       "1          19341               2518           1          21160   \n",
       "2          19233               2230           1          21268   \n",
       "\n",
       "   phase_1_interface  top_boundary_count_0  top_boundary_count_1  \n",
       "0               2729                    54                    47  \n",
       "1               2564                    47                    54  \n",
       "2               2278                    49                    52  \n",
       "\n",
       "[3 rows x 40 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_descriptors(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbb4193",
   "metadata": {},
   "source": [
    "The following demonstrates how to use the graph descriptors to classify microstructures. \n",
    "\n",
    "Here, two categories of microstructure are generated each with 96 samples using a Cahn-Hilliard simulation. The two categories of microstructures differ based on the duration of evolution (10 steps versus 100 steps). This is not a particularly useful machine learning example, but suffices to demonstrate using the graph descriptors alongside Scikit-learn.\n",
    "\n",
    "The `generate_data` function uses the PyMKS function `solve_cahn_hilliard` to generate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec804ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n_category, n_chunks, n_domain, seed=99):\n",
    "    \"\"\"Generate the Cahn-Hilliard data\"\"\"\n",
    "    da.random.seed(seed)\n",
    "    solve_ch = curry(solve_cahn_hilliard)(delta_t=1.0, delta_x=0.5)\n",
    "    x_data_ = pipe(\n",
    "        da.random.random(\n",
    "            (n_category * 2, n_domain, n_domain), chunks=(n_chunks, n_domain, n_domain)\n",
    "        ),\n",
    "        lambda x: 2 * x - 1,\n",
    "        lambda x: [\n",
    "            solve_ch(x[:n_category], n_steps=10),\n",
    "            solve_ch(x[n_category:], n_steps=100),\n",
    "        ],\n",
    "        da.concatenate,\n",
    "        lambda x: da.where(x > 0, 1, 0).persist(),\n",
    "    )\n",
    "    y_data_ = da.from_array(\n",
    "        np.concatenate([np.zeros(n_category), np.ones(n_category)]).astype(int),\n",
    "        chunks=(n_chunks,),\n",
    "    )\n",
    "    return np.array(x_data_), np.array(y_data_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67c101a",
   "metadata": {},
   "source": [
    "Below, `n_category` refers to the number of samples per category, `n_chunks` refers to the number of samples per chunk of data in the Dask array, `n_domain` refers to the number of pixels along an edge of the domain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "132d5917",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data, y_data = generate_data(n_category=96, n_chunks=24, n_domain=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdabff1",
   "metadata": {},
   "source": [
    "The following generates the graph descriptors from the raw microstructures. Note that only `_make_skeletal_descriptors` is used as the graph descriptors are inefficient in the current version of PyGraSPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b50b63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace this with make_descriptors when switched over to use Graph-tool\n",
    "x_graph = make_descriptors(x_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906abcf2",
   "metadata": {},
   "source": [
    "The redundant, constant-value features need to be removed otherwise the `LogisticRegression` fails to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e833d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ~x_graph.eq(x_graph.iloc[0]).all()\n",
    "x_graph_clean = x_graph.loc[:, mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c233b081",
   "metadata": {},
   "source": [
    "Train / test split the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d30cda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_graph, x_test_graph, y_train, y_test = train_test_split(\n",
    "    np.array(x_graph_clean), y_data, test_size=0.2, random_state=99\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4f46d1",
   "metadata": {},
   "source": [
    "The graph data is required to be scaled for the logistic regression. Note that the scaler is only fit using the training data (not all the data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f395f81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train_graph)\n",
    "x_test_scaled = scaler.transform(x_test_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359e9b31",
   "metadata": {},
   "source": [
    "train ther regresson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a57031d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_IGNORE_OUTPUT\n",
    "model = LogisticRegression().fit(x_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5b52792",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = model.predict(x_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03667b69",
   "metadata": {},
   "source": [
    "This is a very easy classification problem and so the predictions are perfect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bd8d547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[22,  0],\n",
       "       [ 0, 17]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_predict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
